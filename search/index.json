[{"content":"简述 ​\t就以我交实验作业的爬虫代码作为2024年的第一篇博客，也作为我更新博客的开端。其实这个爬虫并不是实验要求的，而是我舍友要我写的，因为我们写的是图书管理系统，所以就把爬数据的任务交给我了。如果想要代码可直接翻到最下面。\n思路 要爬取的页面 要爬取的数据格式 存储数据 代码 import re import time import requests import openpyxl findLink = re.compile(r\u0026#39;\u0026lt;a href=\\\u0026#34;https://book.douban.com/subject/(.*?)\\\u0026#34;\u0026#39;) #获取图书的图书的链接 findTitle = re.compile(r\u0026#39;\u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;(.*?)\u0026#34; /\u0026gt;\u0026#39;) #获取图书的名称 findImgSrc = re.compile(r\u0026#39;\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;(.*?)\u0026#34; /\u0026gt;\u0026#39;, re.S) #获取图书图片的链接 findDate = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;pl\u0026#34;\u0026gt;出版年:\u0026lt;/span\u0026gt; (.*?)\u0026lt;br/\u0026gt;\u0026#39;) #图书出版日期 findAuthor = re.compile(r\u0026#39;\u0026lt;meta property=\u0026#34;book:author\u0026#34; content=\u0026#34;(.*?)\u0026#34; /\u0026gt;\u0026#39;) #图书作者名称 findCode = re.compile(r\u0026#39;\u0026lt;meta property=\u0026#34;book:isbn\u0026#34; content=\u0026#34;(.*?)\u0026#34; /\u0026gt;\u0026#39;) #图书编码 findDes = re.compile(r\u0026#39;\u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;(.*?)\u0026#34; /\u0026gt;\u0026#39;, re.S) #图书描述 findPublish = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;pl\u0026#34;\u0026gt;出版社:\u0026lt;/span\u0026gt;(.*?)\u0026lt;a href=(.*?)\u0026gt;(.*?)\u0026lt;/a\u0026gt;\u0026#39;, re.S) # 图书出版商 #创建一个新的工作簿对象 workbook = openpyxl.Workbook() # 获取默认的工作表 sheet = workbook.active # 写入标题行 sheet.append([\u0026#34;名称\u0026#34;, \u0026#34;描述\u0026#34;, \u0026#34;出版日期\u0026#34;, \u0026#34;作者\u0026#34;, \u0026#34;出版社\u0026#34;, \u0026#34;标准码\u0026#34;, \u0026#34;封面\u0026#34;]) workbook.save(\u0026#34;data.xlsx\u0026#34;) def save(data): sheet.append(data) workbook.save(\u0026#34;data.xlsx\u0026#34;) def askUrl(url): head = { # 模拟浏览器头部信息，向豆瓣服务器发送消息 \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\u0026#34; } html = \u0026#34;\u0026#34; try: resp = requests.get(url, headers=head) html = resp.text print(resp.status_code) except requests.exceptions.RequestException as e: print(\u0026#34;An error occurred\u0026#34;, str(e)) return html def getData(baseurl): count = 0 for i in range(0, 300): url = baseurl + str(i * 20) html = askUrl(url) links = re.findall(findLink, html) for link in list(filter(lambda link: \u0026#34;buylinks\u0026#34; not in link, links)): newurl = \u0026#34;https://book.douban.com/subject/\u0026#34; + link time.sleep(5) html = askUrl(newurl) data = [] try: count += 1 # 书名 title = re.findall(findTitle, html)[0] #图片链接 imgsrc = re.findall(findImgSrc, html)[0] #出版日期 date = re.findall(findDate, html)[0] #图书作者 author = re.findall(findAuthor, html)[0] #出版商 publish = re.findall(findPublish, html)[0][2] #编号 code = re.findall(findCode, html)[0] #描述 decr = re.findall(findDes, html)[0] data.append(title) data.append(decr) data.append(date) data.append(author) data.append(publish) data.append(f\u0026#34;ISBN: {code}\u0026#34;) data.append(imgsrc) save(data) if count == 1000: exit(\u0026#34;爬取完成\u0026#34;) except: continue def main(): baseurl = \u0026#34;https://book.douban.com/tag/经典?type=T\u0026amp;start=\u0026#34; getData(baseurl) if __name__ == \u0026#39;__main__\u0026#39;: main() ​\t运行之后，常常会出现403码，估计还是访问太快导致的，不过当时要求只是一定量的数据即可，所以有些数据没爬到就算了。\n​\t本来想长篇大论的详细说说代码是怎么写的，但里面知识点挺多的，全说的话不知道该怎么讲。还是后面有时间写个基础爬虫的文章算了。\n","date":"2024-01-16T17:48:22+08:00","image":"https://gcore.jsdelivr.net/gh/wingllllet/pic_bed@img//img/640.jpg","permalink":"https://blog.winglet.com/p/%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E5%9B%BE%E4%B9%A6%E4%BF%A1%E6%81%AF/","title":"爬取豆瓣图书信息"},{"content":"ok，我的2024年第一篇博客就此展开，回顾一下我的2023年，这一年里我领到了校赛证书，互联网+证书，认识了一群大佬，并且通过我舍友介绍去挣了大学第一桶金。不过总的来看，这一年还是一个摆烂的一年。不过这两年走过来，我也有了一定的方向，希望2024年我能坚持学习下去。完成2024年的目标。 ","date":"2024-01-15T11:46:02+08:00","image":"https://gcore.jsdelivr.net/gh/wingllllet/pic_bed@img/img/2.jpg","permalink":"https://blog.winglet.com/p/hello-world/","title":"Hello World"}]